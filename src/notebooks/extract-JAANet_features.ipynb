{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.util import torch_fix_seed, get_video_name_list\n",
    "from dataset import AUImageList\n",
    "from preprocess import JAANet_ImageTransform\n",
    "from networks import JAANet_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AUImageList(\n",
    "        labels_path=\"/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/labels/PIMD_A/emo_and_au(video1-25).csv\",\n",
    "        video_name_list=get_video_name_list('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/labels/PIMD_A/emo_and_au(video1-25)-video_name_list.csv'),\n",
    "        au_transform=JAANet_ImageTransform(phase='test')\n",
    "    )\n",
    "    \n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device:cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch_fix_seed()\n",
    "\n",
    "device = (torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"use device:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* JAANet(AU Estimator)\n",
    "region_learning = JAANet_networks.network_dict['HMRegionLearning'](input_dim=3, unit_dim=8)\n",
    "region_learning.load_state_dict(torch.load('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/params/JAANet-snapshots/region_learning.pth', map_location=device))\n",
    "\n",
    "align_net = JAANet_networks.network_dict['AlignNet'](crop_size=176, map_size=44, au_num=12, land_num=49, input_dim=64, fill_coeff=0.56)\n",
    "align_net.load_state_dict(torch.load('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/params/JAANet-snapshots/align_net.pth', map_location=device))\n",
    "\n",
    "local_attention_refine = JAANet_networks.network_dict['LocalAttentionRefine'](au_num=12, unit_dim=8)\n",
    "local_attention_refine.load_state_dict(torch.load('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/params/JAANet-snapshots/local_attention_refine.pth', map_location=device))\n",
    "    \n",
    "local_au_net = JAANet_networks.network_dict['LocalAUNetv1'](au_num=12, input_dim=64, unit_dim=8)\n",
    "local_au_net.load_state_dict(torch.load('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/params/JAANet-snapshots/local_au_net.pth', map_location=device))\n",
    "    \n",
    "global_au_feat = JAANet_networks.network_dict['HLFeatExtractor'](input_dim=64, unit_dim=8)           \n",
    "global_au_feat.load_state_dict(torch.load('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/params/JAANet-snapshots/global_au_feat.pth', map_location=device))\n",
    "\n",
    "au_net = JAANet_networks.network_dict['AUNet'](au_num=12, input_dim=12000, unit_dim=8)\n",
    "au_net.load_state_dict(torch.load('/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/params/JAANet-snapshots/au_net.pth', map_location=device))\n",
    "\n",
    "au_net_layer0 = JAANet_networks.network_dict['AUNet_0'](input_dim=12000, unit_dim=8)\n",
    "for param_t, param_s in zip(au_net_layer0.parameters(), au_net.au_output[0].parameters()):\n",
    "    param_t.data = param_s.data\n",
    "    \n",
    "au_net_layer1 = JAANet_networks.network_dict['AUNet_1'](au_num=12, input_dim=512)\n",
    "for param_t, param_s in zip(au_net_layer1.parameters(), au_net.au_output[1].parameters()):\n",
    "    param_t.data = param_s.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_learning is trainable: False\n",
      "align_net is trainable: False\n",
      "local_attention_refine is trainable: False\n",
      "local_au_net is trainable: False\n",
      "global_au_feat is trainable: False\n",
      "au_net_layer0 is trainable: False\n",
      "au_net_layer1 is trainable: False\n"
     ]
    }
   ],
   "source": [
    "module_dict = {\n",
    "    'region_learning': region_learning, \n",
    "    'align_net': align_net, \n",
    "    'local_attention_refine': local_attention_refine, \n",
    "    'local_au_net': local_au_net, \n",
    "    'global_au_feat': global_au_feat,\n",
    "    'au_net_layer0': au_net_layer0,\n",
    "    'au_net_layer1': au_net_layer1\n",
    "}\n",
    "    \n",
    "training_module_list = []\n",
    "       \n",
    "for module_name, module in module_dict.items():\n",
    "    if module_name not in training_module_list:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "                    \n",
    "            module.eval()\n",
    "\n",
    "for module_name, module in module_dict.items():\n",
    "        print(f\"{module_name} is trainable: {module.training}\")\n",
    "\n",
    "region_learning = region_learning.to(device)\n",
    "align_net = align_net.to(device)\n",
    "local_attention_refine = local_attention_refine.to(device)\n",
    "local_au_net = local_au_net.to(device)\n",
    "global_au_feat = global_au_feat.to(device)\n",
    "au_net_layer0 = au_net_layer0.to(device)\n",
    "au_net_layer1 = au_net_layer1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19931/19931 [30:55<00:00, 10.74it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(dataloader))\n",
    "\n",
    "middle_features = []\n",
    "middle_features_512 = []\n",
    "au_logits = []\n",
    "au_posteriors = []\n",
    "img_path_list = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        imgs, img_paths, emos, _ = batch\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # culc forward\n",
    "        region_feat = region_learning(imgs)\n",
    "        align_feat, align_output, aus_map = align_net(region_feat)\n",
    "        aus_map = aus_map.to(device)\n",
    "        output_aus_map = local_attention_refine(aus_map.detach())\n",
    "        local_au_out_feat = local_au_net(region_feat, output_aus_map)\n",
    "        global_au_out_feat = global_au_feat(region_feat)\n",
    "        concat_au_feat = torch.cat((align_feat, global_au_out_feat, local_au_out_feat), dim=1)\n",
    "        concat_au_feat = concat_au_feat.view(concat_au_feat.size(0), -1)\n",
    "        middle_features += concat_au_feat.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        au_net_layer0_outputs = au_net_layer0(concat_au_feat)\n",
    "        middle_features_512 += au_net_layer0_outputs.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        au_net_outputs = au_net_layer1(au_net_layer0_outputs)\n",
    "        au_logits += au_net_outputs.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        au_net_outputs = au_net_outputs.view(au_net_outputs.size(0), 2, int(au_net_outputs.size(1)/2))\n",
    "        au_net__outputs = torch.softmax(au_net_outputs, dim=1)\n",
    "        au_net_outputs = au_net_outputs[:,1,:]\n",
    "        au_posteriors += au_net_outputs.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # save outputs\n",
    "        img_path_list += img_paths\n",
    "        \n",
    "        # update tqdm bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# close tqdm bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = pd.DataFrame(img_path_list, columns=[\"img_path\"])\n",
    "df_au = pd.DataFrame(au_posteriors, columns=[\"AU01\", \"AU02\", \"AU04\", \"AU06\", \"AU07\", \"AU10\", \"AU12\", \"AU14\", \"AU15\", \"AU17\", \"AU23\", \"AU24\"])\n",
    "df_mid = pd.DataFrame(middle_features, columns=[i for i in range(12000)])\n",
    "df_mid_512 = pd.DataFrame(middle_features_512, columns=[i for i in range(512)])\n",
    "df_logits = pd.DataFrame(au_logits, columns=[i for i in range(24)])\n",
    "\n",
    "au_list = pd.concat([df_path, df_au], axis=1)\n",
    "mid_list = pd.concat([df_path, df_mid], axis=1)\n",
    "mid512_list = pd.concat([df_path, df_mid_512], axis=1)\n",
    "logits_list = pd.concat([df_path, df_logits], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid_list.to_pickle(\"/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/processed/JAANet_feature.pkl\")\n",
    "au_list.to_pickle(\"/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/processed/PIMD_A/JAANet_posterior.pkl\")\n",
    "mid512_list.to_pickle(\"/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/processed/PIMD_A/JAANet_feature_512.pkl\")\n",
    "logits_list.to_pickle(\"/mnt/iot-qnap3/mochida/medical-care/emotionestimation/data/processed/PIMD_A/JAANet_logits.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
